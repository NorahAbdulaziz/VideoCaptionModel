{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYODqORR8C3ajG05K9RWyx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NorahAbdulaziz/VideoCaptionModel/blob/main/Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ],
      "metadata": {
        "id": "vV2aAvnNJmIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/Shreyz-max/Video-Captioning.git"
      ],
      "metadata": {
        "id": "U5sI4tPbKDm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/Video-Captioning/requirements.txt"
      ],
      "metadata": {
        "id": "Zi3bGV45KGWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os, sys\n",
        "import pickle, functools, operator\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import joblib\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import json\n",
        "import random\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import tensorflow as tf\n",
        "import datetime"
      ],
      "metadata": {
        "id": "21UynMaRGvFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#source code:\n",
        "https://github.com/TaherAmeen/Video-Captioning/blob/main/Video_Captioning.ipynb\n",
        "\n",
        "https://github.com/Shreyz-max/Video-Captioning"
      ],
      "metadata": {
        "id": "Km04Zgy1KcZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "uQRHBIXvJaum"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSapAEJRCA0C"
      },
      "outputs": [],
      "source": [
        "TRAIN_LABEL_PATH = os.path.join('TrainingLabelPath')\n",
        "VAL_LABEL_PATH = os.path.join('ValidationLabelPath')\n",
        "\n",
        "with open(TRAIN_LABEL_PATH) as data_filetrain:    \n",
        "    y_datatrain = json.load(data_filetrain)\n",
        "with open(VAL_LABEL_PATH) as data_fileval:    \n",
        "    y_dataval = json.load(data_fileval)\n",
        "\n",
        "train_list = []\n",
        "valid_list = []\n",
        "\n",
        "vocab_list = []\n",
        "for ytrain in y_datatrain:\n",
        "  for captiontrain in ytrain['caption']:\n",
        "      train_list.append([captiontrain, ytrain['id']])\n",
        "print(\"trainList\",len(train_list))\n",
        "\n",
        "for yvalid in y_dataval:\n",
        "  for captionvalid in yvalid['caption']:\n",
        "      valid_list.append([captionvalid, yvalid['id']])\n",
        "print(\"ValidList\",len(valid_list))\n",
        "\n",
        "\n",
        "training_list = train_list\n",
        "validation_list = valid_list\n",
        "for train in training_list:\n",
        "    vocab_list.append(train[0])\n",
        "# Tokenizing the words\n",
        "tokenizer = Tokenizer(num_words=6000)#6000 for English, 8000 for Arabic\n",
        "tokenizer.fit_on_texts(vocab_list)\n",
        "print(\"tok\",len(tokenizer.word_index))\n",
        "x_data = {}\n",
        "\n",
        "TRAIN_FEATURE_DIR = os.path.join('TrainingAndValidationFeaturesPath')\n",
        "# Loading all the numpy arrays at once and saving them in a dictionary\n",
        "for filename in os.listdir(TRAIN_FEATURE_DIR):\n",
        "    f = np.load(os.path.join(TRAIN_FEATURE_DIR, filename))\n",
        "    x_data[filename[:-4]] = f\n",
        "\n",
        "print(\"trainingList\",len(training_list))\n",
        "print(\"valList\",len(validation_list))\n",
        "print(len(x_data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a custom data generator because we cannot load so many files at once\n",
        "def load_datatest(train_path, epochs=100, x_data=x_data, tokenizer=tokenizer, num_decoder_tokens=6000,training_list=train_list, batch_size=32, maxlen=30):\n",
        "    encoder_input_data = []\n",
        "    decoder_input_data = []\n",
        "    decoder_target_data = []\n",
        "    videoId = []\n",
        "    videoSeq = []\n",
        "    # separating the videoId and the video captions\n",
        "    for idx, cap in enumerate(training_list):\n",
        "        caption = cap[0]\n",
        "        videoId.append(cap[1])\n",
        "        videoSeq.append(caption)\n",
        "    # converting the captions to tokens and padding them to equal sizes\n",
        "    train_sequences = tokenizer.texts_to_sequences(videoSeq)\n",
        "    train_sequences = np.array(train_sequences)\n",
        "    train_sequences = pad_sequences(train_sequences, padding='post',truncating='post', maxlen=maxlen)\n",
        "    max_seq_length = train_sequences.shape[1]\n",
        "    filesize = len(train_sequences)\n",
        "    X_data = []\n",
        "    y_data = []\n",
        "    vCount = 0\n",
        "    n = 0\n",
        "    for i in range(epochs):\n",
        "      for idx in  range(0,filesize):\n",
        "        n += 1\n",
        "        encoder_input_data.append(x_data[videoId[idx]])\n",
        "        y = to_categorical(train_sequences[idx], num_decoder_tokens)\n",
        "        decoder_input_data.append(y[:-1])\n",
        "        decoder_target_data.append(y[1:])\n",
        "        if n == batch_size:\n",
        "          encoder_input = np.array(encoder_input_data)\n",
        "          decoder_input = np.array(decoder_input_data)\n",
        "          decoder_target = np.array(decoder_target_data)\n",
        "          encoder_input_data = []\n",
        "          decoder_input_data = []\n",
        "          decoder_target_data = []\n",
        "          n = 0\n",
        "          yield ([encoder_input, decoder_input], decoder_target)"
      ],
      "metadata": {
        "id": "VngubP_pGQUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# writing the train and validation generator\n",
        "train = load_datatest(train_path='trainingPath',batch_size=320, training_list=training_list, x_data=x_data, epochs=150)\n",
        "valid = load_datatest(train_path='ValidationPath',batch_size=320, training_list=validation_list, x_data=x_data, epochs=150)"
      ],
      "metadata": {
        "id": "7sAnav8NG-MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "time_steps_encoder is the number of frames per video we will be using for training\n",
        "num_encoder_tokens is the number of features from each frame\n",
        "latent_dim is the number of hidden features for lstm\n",
        "time_steps_decoder is the maximum length of each sentence/content/drive/MyDrive/Extimebased2sim90feats/ex30words6000tok\n",
        "num_decoder_tokens is the final number of tokens in the softmax layer\n",
        "batch size\n",
        "\"\"\"\n",
        "time_steps_encoder=40\n",
        "num_encoder_tokens=4096\n",
        "latent_dim=512\n",
        "time_steps_decoder=30\n",
        "num_decoder_tokens=6000\n",
        "batch_size=320"
      ],
      "metadata": {
        "id": "4HFgVmnIHVRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the encoder\n",
        "encoder_inputs = Input(shape=(time_steps_encoder, num_encoder_tokens), name=\"encoder_inputs\")\n",
        "encoder = LSTM(latent_dim, return_state=True,return_sequences=True, name='endcoder_lstm')\n",
        "_, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "\n",
        "# Set up the decoder\n",
        "decoder_inputs = Input(shape=(time_steps_decoder, num_decoder_tokens), name= \"decoder_inputs\")\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_relu')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.summary()\n",
        "plot_model(model, to_file='model_train.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "8oO8MRMiHatr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early Stopping\n",
        "earlystopping = EarlyStopping(monitor='val_loss', patience = 5, verbose=1, mode='min')\n",
        "\n",
        "# Tensorboard callback\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "# Run training\n",
        "opt = keras.optimizers.Adam(lr = 0.0003)\n",
        "x = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1,patience=2,verbose=0,mode=\"auto\")\n",
        "model.compile(metrics=['accuracy'], optimizer=opt, loss='categorical_crossentropy')\n",
        "\n",
        "try:\n",
        "    model.fit(train, validation_data=valid, validation_steps=(len(validation_list)//batch_size),\n",
        "        epochs=150, steps_per_epoch=(len(training_list)//batch_size),\n",
        "            callbacks=[x, earlystopping, tensorboard_callback])\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nW: interrupt received, stopping\")\n",
        "finally:\n",
        "    pass"
      ],
      "metadata": {
        "id": "JfBEfhf4HwDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(model.history.history['loss'])\n",
        "plt.plot(model.history.history['val_loss'])\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.savefig('loss.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9ssnJ6YHH1PY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(model.history.history['accuracy'])\n",
        "plt.plot(model.history.history['val_accuracy'])\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.savefig('accuracy.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OB8TwuSsH_Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model_path ='ModelPath'\n",
        "if not os.path.exists(save_model_path):\n",
        "    os.makedirs(save_model_path)\n",
        "\n",
        "# Saving encoder as in training\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Saving decoder states and dense layer \n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "encoder_model.summary()\n",
        "decoder_model.summary()\n",
        "encoder_model.save(os.path.join(save_model_path, 'encoder_model.h5'))\n",
        "decoder_model.save_weights(os.path.join(save_model_path, 'decoder_model_weights.h5'))\n",
        "with open(os.path.join(save_model_path,'tokenizer'+ str(num_decoder_tokens) ),'wb') as file:\n",
        "    joblib.dump(tokenizer, file)\n",
        "plot_model(encoder_model, to_file='model_inference_encoder.png', show_shapes=True, show_layer_names=True)\n",
        "plot_model(decoder_model, to_file='model_inference_decoder.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "5wH4ouAbIGtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(encoder_model, to_file='model_inference_encoder.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "Had53oETIUz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caption Generation"
      ],
      "metadata": {
        "id": "Fzl-NQCIIerB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class to perform inference on all test files and save as test_output.txt\n",
        "class Video2Text(object):\n",
        "    ''' Initialize the parameters for the model '''\n",
        "    def __init__(self):\n",
        "        self.latent_dim = 512\n",
        "        self.num_encoder_tokens = 4096\n",
        "        self.num_decoder_tokens = 6000\n",
        "        self.time_steps_encoder = 40\n",
        "        self.time_steps_decoder = None\n",
        "        self.preload = True\n",
        "        self.preload_data_path = 'preload_data'\n",
        "        self.max_probability = -1\n",
        "\n",
        "        # processed data\n",
        "        self.encoder_input_data = []\n",
        "        self.decoder_input_data = []\n",
        "        self.decoder_target_data = []\n",
        "        self.tokenizer = None\n",
        "\n",
        "        # models\n",
        "        self.encoder_model = None\n",
        "        self.decoder_model = None\n",
        "        self.inf_encoder_model = None\n",
        "        self.inf_decoder_model = None\n",
        "        self.save_model_path = 'ModelPath'\n",
        "        self.test_path = 'TestPath'\n",
        "    def load_inference_models(self):\n",
        "        # load tokenizer\n",
        "        \n",
        "        with open(os.path.join(self.save_model_path, 'tokenizer' + str(self.num_decoder_tokens)), 'rb') as file:\n",
        "            self.tokenizer = joblib.load(file)\n",
        "\n",
        "        # inference encoder model\n",
        "        self.inf_encoder_model = load_model(os.path.join(self.save_model_path, 'encoder_model.h5'))\n",
        "\n",
        "        # inference decoder model\n",
        "        decoder_inputs = Input(shape=(None, self.num_decoder_tokens))\n",
        "        decoder_dense = Dense(self.num_decoder_tokens, activation='softmax')\n",
        "        decoder_lstm = LSTM(self.latent_dim, return_sequences=True, return_state=True)\n",
        "        decoder_state_input_h = Input(shape=(self.latent_dim,))\n",
        "        decoder_state_input_c = Input(shape=(self.latent_dim,))\n",
        "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "        decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "        decoder_states = [state_h, state_c]\n",
        "        decoder_outputs = decoder_dense(decoder_outputs)\n",
        "        self.inf_decoder_model = Model(\n",
        "            [decoder_inputs] + decoder_states_inputs,\n",
        "            [decoder_outputs] + decoder_states)\n",
        "        self.inf_decoder_model.load_weights(os.path.join(self.save_model_path, 'decoder_model_weights.h5'))\n",
        "    \n",
        "    def decode_sequence2bs(self, input_seq):\n",
        "        states_value = self.inf_encoder_model.predict(input_seq)\n",
        "        target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n",
        "        target_seq[0, 0, self.tokenizer.word_index['bos']] = 1\n",
        "        self.beam_search(target_seq, states_value,[],[],0)\n",
        "        return decode_seq\n",
        "\n",
        "    def beam_search(self, target_seq, states_value, prob,  path, lens):\n",
        "        global decode_seq\n",
        "        node = 2\n",
        "        output_tokens, h, c = self.inf_decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "        output_tokens = output_tokens.reshape((self.num_decoder_tokens))\n",
        "        sampled_token_index = output_tokens.argsort()[-node:][::-1]\n",
        "        states_value = [h, c]\n",
        "        for i in range(node):\n",
        "            if sampled_token_index[i] == 0:\n",
        "                sampled_char = ''\n",
        "            else:\n",
        "                sampled_char = list(self.tokenizer.word_index.keys())[list(self.tokenizer.word_index.values()).index(sampled_token_index[i])]\n",
        "            MAX_LEN = 10\n",
        "            if(sampled_char != 'eos' and lens <= MAX_LEN):\n",
        "                p = output_tokens[sampled_token_index[i]]\n",
        "                if(sampled_char == ''):\n",
        "                    p = 1\n",
        "                prob_new = list(prob)\n",
        "                prob_new.append(p)\n",
        "                path_new = list(path)\n",
        "                path_new.append(sampled_char)\n",
        "                target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n",
        "                target_seq[0, 0, sampled_token_index[i]] = 1.\n",
        "                self.beam_search(target_seq, states_value, prob_new, path_new, lens+1)\n",
        "            else:\n",
        "                p = output_tokens[sampled_token_index[i]]\n",
        "                prob_new = list(prob)\n",
        "                prob_new.append(p)\n",
        "                p = functools.reduce(operator.mul, prob_new, 1)\n",
        "                if(p > self.max_probability):\n",
        "                    decode_seq = path\n",
        "                    self.max_probability = p\n",
        "\n",
        "    def decoded_sentence_tuning(self, decoded_sentence):\n",
        "        decode_str = []\n",
        "        filter_string = ['bos', 'eos']\n",
        "        unigram = {}\n",
        "        last_string = \"\"\n",
        "        for idx2, c in enumerate(decoded_sentence):\n",
        "            if c in unigram:\n",
        "                unigram[c] += 1\n",
        "            else:\n",
        "                unigram[c] = 1\n",
        "            if(last_string == c and idx2 > 0):\n",
        "                continue\n",
        "            if c in filter_string:\n",
        "                continue\n",
        "            if len(c) > 0:\n",
        "                decode_str.append(c)\n",
        "            if idx2 > 0:\n",
        "                last_string = c\n",
        "        return decode_str\n",
        "\n",
        "    def get_test_data(self, path):\n",
        "        X_test = []\n",
        "        X_test_filename = []\n",
        "        with open (os.path.join('PathOfTestIds')) as testing_file:\n",
        "            lines = testing_file.readlines()\n",
        "            for filename in lines:\n",
        "                filename= filename.strip(\"\\n\") \n",
        "       \n",
        "                print(filename)\n",
        "                f = np.load(os.path.join(test_path, 'Feat', filename+'.npy'))\n",
        "                X_test.append(f)\n",
        "                X_test_filename.append(filename)\n",
        "            X_test = np.array(X_test)\n",
        "        return X_test, X_test_filename\n",
        "\n",
        "    def test(self):\n",
        "        X_test, X_test_filename = self.get_test_data(os.path.join(self.test_path))\n",
        "        print(len(X_test), len(X_test_filename))\n",
        "        # generate inference test outputs\n",
        "        with open(os.path.join(test_path, 'test_output.txt'), 'w') as file:\n",
        "            for idx, x in enumerate(X_test): \n",
        "                file.write(X_test_filename[idx]+',')\n",
        "                decoded_sentence = self.decode_sequence2bs(x.reshape(-1, 40, 4096))\n",
        "                decode_str = self.decoded_sentence_tuning(decoded_sentence)\n",
        "                for d in decode_str:\n",
        "                    file.write(d + ' ')\n",
        "                file.write('\\n')\n",
        "                # re-init max prob\n",
        "                self.max_probability = -1\n"
      ],
      "metadata": {
        "id": "5iqSziJdIfaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = Video2Text()\n",
        "c.load_inference_models()\n",
        "c.test()"
      ],
      "metadata": {
        "id": "IaAwN7IRJUkB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}