{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDedVGjupn/AHWpy0p8JJh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NorahAbdulaziz/VideoCaptionModel/blob/main/VideoCaptioningEvaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Translation-based"
      ],
      "metadata": {
        "id": "edRxX5wJLIN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requirements"
      ],
      "metadata": {
        "id": "ybgG3fx7MgIs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PM4Gzm1ALBcH"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/Maluuba/nlg-eval.git@master"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "g4_SAJrHL6KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nlgeval.pycocoevalcap.bleu.bleu import Bleu\n",
        "from nlgeval.pycocoevalcap.rouge.rouge import Rouge\n",
        "from nlgeval.pycocoevalcap.cider.cider import Cider\n",
        "from nlgeval.pycocoevalcap.meteor.meteor import Meteor"
      ],
      "metadata": {
        "id": "hjFsuP2NMVRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source code:\n",
        "https://github.com/Maluuba/nlg-eval"
      ],
      "metadata": {
        "id": "E1L0ipSCMpgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hypo_df = pd.read_csv('generatedCaptionsFile', header=None)\n",
        "hypo_df.columns = ['id', 'text']\n",
        "print(hypo_df.shape)\n",
        "hypo_df.head()\n"
      ],
      "metadata": {
        "id": "LoS-eHJYL8aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ref_df = pd.read_json('refrenceCaptionsFile')\n",
        "print(ref_df.shape)\n",
        "ref_df.head()"
      ],
      "metadata": {
        "id": "pLBiFqMwL-bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hypo = {}\n",
        "for index, row in hypo_df.iterrows():\n",
        "    hypo[row['id']] =  [row['text']]\n",
        "\n",
        "len(hypo)"
      ],
      "metadata": {
        "id": "2uOAEOwdMFzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ref = {}\n",
        "for index, row in ref_df.iterrows():\n",
        "    ref[row['id']] =  row['caption']\n",
        "\n",
        "len(ref)"
      ],
      "metadata": {
        "id": "zeW0n25DMHyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ref.keys() == hypo.keys()"
      ],
      "metadata": {
        "id": "KzEbq0iHMJvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score(ref, hypo):\n",
        "    scorers = [\n",
        "      (Bleu(4),[\"Bleu_1\",\"Bleu_2\",\"Bleu_3\",\"Bleu_4\"]),\n",
        "      (Meteor(),\"METEOR\"),\n",
        "      (Rouge(),\"ROUGE_L\"),\n",
        "      (Cider(),\"CIDEr\")\n",
        "  ]\n",
        "    final_scores = {}\n",
        "    for scorer,method in scorers:\n",
        "        score,scores = scorer.compute_score(ref, hypo)\n",
        "        if type(score)==list:\n",
        "            for m,s in zip(method,score):\n",
        "                final_scores[m] = s\n",
        "        else:\n",
        "            final_scores[method] = score\n",
        "\n",
        "    return final_scores\n"
      ],
      "metadata": {
        "id": "1hwBmiEOMPxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute score\n",
        "final_scores = score(ref, hypo)\n",
        "\n",
        "# print out scores\n",
        "print('Bleu_1:\\t', final_scores['Bleu_1'])\n",
        "print('Bleu_2:\\t', final_scores['Bleu_2'])\n",
        "print('Bleu_3:\\t', final_scores['Bleu_3'])\n",
        "print('Bleu_4:\\t', final_scores['Bleu_4'])\n",
        "print('METEOR:\\t', final_scores['METEOR'])\n",
        "print('ROUGE_L:', final_scores['ROUGE_L'])\n",
        "print('CIDEr:\\t', final_scores['CIDEr'])"
      ],
      "metadata": {
        "id": "pRk66AvfMSNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Video Retrieval-based"
      ],
      "metadata": {
        "id": "-Zqrz6dsLkgZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requirements"
      ],
      "metadata": {
        "id": "AVtv8VQNRgJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install nltk"
      ],
      "metadata": {
        "id": "mBt098sZRYbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')\n",
        "pip install Arabic-Stopwords #from:https://pypi.org/project/Arabic-Stopwords/"
      ],
      "metadata": {
        "id": "nJTpVF2NMFNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords #english\n",
        "import arabicstopwords.arabicstopwords as stp #arabic\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import json, os"
      ],
      "metadata": {
        "id": "k98Ka1NvRMO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source Code:\n",
        "https://www.geeksforgeeks.org/python-measure-similarity-between-two-sentences-using-cosine-similarity/"
      ],
      "metadata": {
        "id": "54o3lPSmRmgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#English Version\n",
        "TRAIN_LABEL_PATH = os.path.join('generatedCaptionsFile')\n",
        "Test_LABEL_PATH = os.path.join('refrenceCaptionsFile')\n",
        "count=0\n",
        "\n",
        "\n",
        "test= open(Test_LABEL_PATH) \n",
        "data_file= open(TRAIN_LABEL_PATH)\n",
        "\n",
        "y_data = json.load(test)\n",
        "\n",
        "for k in data_file:\n",
        "\n",
        "    X = k.split(',')[1]\n",
        "    X = re.sub(r'[^\\w\\s]', '', X)\n",
        "    for j in y_data:\n",
        "\n",
        "      for caption in j['caption']:\n",
        "        #Y = j.split(',')[0]\n",
        "        Y = re.sub(r'[^\\w\\s]', '', caption)\n",
        "\n",
        "        # tokenization\n",
        "        X_list = word_tokenize(X)\n",
        "        Y_list = word_tokenize(Y)\n",
        "\n",
        "        # sw contains the list of stopwords\n",
        "        sw = stopwords.words('english')\n",
        "        l1 =[];l2 =[]\n",
        "\n",
        "        # remove stop words from the string\n",
        "        X_set = {w for w in X_list if not w in sw}\n",
        "        Y_set = {w for w in Y_list if not w in sw}\n",
        "\n",
        "        # form a set containing keywords of both strings\n",
        "        rvector = X_set.union(Y_set)\n",
        "      # print(rvector)\n",
        "        for w in rvector:\n",
        "          if w in X_set: l1.append(1) # create a vector\n",
        "          else: l1.append(0)\n",
        "          if w in Y_set: l2.append(1)\n",
        "          else: l2.append(0)\n",
        "      c = 0\n",
        "\n",
        "      # cosine formula\n",
        "      for i in range(len(rvector)):\n",
        "          c+= l1[i]*l2[i]\n",
        "\n",
        "      cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
        "      if(cosine > .30  and k.split(',')[0]==j['id']): #threshold equals 30\n",
        "          count=count+1\n"
      ],
      "metadata": {
        "id": "rIkClWkQQf4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Arabic Version\n",
        "TRAIN_LABEL_PATH = os.path.join('generatedCaptionsFile')\n",
        "Test_LABEL_PATH = os.path.join('refrenceCaptionsFile')\n",
        "count=0\n",
        "\n",
        "\n",
        "test= open(Test_LABEL_PATH) \n",
        "data_file= open(TRAIN_LABEL_PATH)\n",
        "\n",
        "y_data = json.load(test)\n",
        "\n",
        "for k in data_file:\n",
        "\n",
        "    X = k.split(',')[1]\n",
        "    X = re.sub(r'[^\\w\\s]', '', X)\n",
        "    for j in y_data:\n",
        "\n",
        "      for caption in j['caption']:\n",
        "        #Y = j.split(',')[0]\n",
        "        Y = re.sub(r'[^\\w\\s]', '', caption)\n",
        "\n",
        "        # tokenization\n",
        "        X_list = word_tokenize(X)\n",
        "        Y_list = word_tokenize(Y)\n",
        "\n",
        "        l1 =[];l2 =[]\n",
        "\n",
        "        # remove stop words from the string\n",
        "        X_set = {w for w in X_list if not stp.is_stop(w) is True}\n",
        "        Y_set = {w for w in Y_list if not stp.is_stop(w) is True}\n",
        "\n",
        "        # form a set containing keywords of both strings\n",
        "        rvector = X_set.union(Y_set)\n",
        "\n",
        "        for w in rvector:\n",
        "          if w in X_set: l1.append(1) # create a vector\n",
        "          else: l1.append(0)\n",
        "          if w in Y_set: l2.append(1)\n",
        "          else: l2.append(0)\n",
        "      c = 0\n",
        "\n",
        "      # cosine formula\n",
        "      for i in range(len(rvector)):\n",
        "          c+= l1[i]*l2[i]\n",
        "\n",
        "      cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
        "      if(cosine > .30  and k.split(',')[0]==j['id']): #threshold equals 30\n",
        "          count=count+1\n"
      ],
      "metadata": {
        "id": "3qfgr8tfSPW2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}